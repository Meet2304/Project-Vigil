{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Solution: Text Analysis Without Pre-trained Vectorizer v0.8\n",
    "## Project Vigil - K-Gram Analysis with Proper Model Training\n",
    "\n",
    "## **The Real Problem & Complete Solution**\n",
    "\n",
    "### What Went Wrong in v0.5-v0.7?\n",
    "\n",
    "‚ùå **You have**: `classifier.pkl` (the model)\n",
    "‚ùå **You DON'T have**: `vectorizer.pkl` (the text-to-numbers converter)\n",
    "‚ùå **Result**: Model expects specific features, gets random ones ‚Üí Doesn't work!\n",
    "\n",
    "### Solution in v0.8:\n",
    "\n",
    "We'll implement **3 approaches** so you can choose the best one:\n",
    "\n",
    "1. ‚úÖ **Retrain Model** (RECOMMENDED) - Train new model + vectorizer together\n",
    "2. ‚úÖ **LIME Text Explainer** - Model-agnostic, works on raw text\n",
    "3. ‚úÖ **K-Gram Statistical Analysis** - No model needed, just patterns\n",
    "\n",
    "### Author: Project Vigil Team\n",
    "### Version: 0.8 (Complete Solution)\n",
    "### Date: 2025-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn tqdm xgboost lime\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# LIME for explanations\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "GITHUB_REPO = \"https://raw.githubusercontent.com/Meet2304/Project-Vigil/claude/fix-kgram-dataset-01VTpiw6P21u1bbgrvx2rVb2\"\n",
    "DATASET_URL = f\"{GITHUB_REPO}/Dataset/MPDD.csv\"\n",
    "\n",
    "# Config\n",
    "CONFIG = {\n",
    "    'sample_size': 10000,      # Use 10K for good coverage\n",
    "    'test_size': 0.2,          # 20% for testing\n",
    "    'random_state': 42,\n",
    "    'ngram_range': (1, 3),     # 1-3 word n-grams\n",
    "    'max_features': 5000,      # Top 5000 features\n",
    "    'min_df': 2,               # Must appear in at least 2 docs\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "def download_file(url, path):\n",
    "    try:\n",
    "        ssl_context = ssl.create_default_context()\n",
    "        ssl_context.check_hostname = False\n",
    "        ssl_context.verify_mode = ssl.CERT_NONE\n",
    "        print(f\"Downloading {url}...\")\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        print(f\"‚úì Downloaded\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        return False\n",
    "\n",
    "if not os.path.exists(\"MPDD.csv\"):\n",
    "    download_file(DATASET_URL, \"MPDD.csv\")\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(\"MPDD.csv\")\n",
    "print(f\"\\n‚úì Loaded {len(df):,} samples\")\n",
    "\n",
    "# Sample\n",
    "if CONFIG['sample_size'] < len(df):\n",
    "    df = df.sample(n=CONFIG['sample_size'], random_state=CONFIG['random_state'], \n",
    "                   stratify=df['isMalicious']).reset_index(drop=True)\n",
    "\n",
    "texts = df['Prompt'].astype(str).tolist()\n",
    "labels = df['isMalicious'].astype(int).values\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"  Total: {len(texts):,}\")\n",
    "print(f\"  Malicious: {sum(labels):,} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"  Benign: {len(labels)-sum(labels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 1: Retrain Model with Matching Vectorizer (RECOMMENDED)\n",
    "\n",
    "### Why This is Best:\n",
    "- ‚úÖ Model and vectorizer are guaranteed to match\n",
    "- ‚úÖ You can save both for future use\n",
    "- ‚úÖ Full control over features\n",
    "- ‚úÖ Can use SHAP properly\n",
    "\n",
    "### Steps:\n",
    "1. Create vectorizer\n",
    "2. Transform text to features\n",
    "3. Train new model\n",
    "4. Save BOTH model and vectorizer\n",
    "5. Analyze with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"APPROACH 1: RETRAIN MODEL WITH MATCHING VECTORIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train):,}, Test: {len(X_test):,}\")\n",
    "\n",
    "# Create vectorizer\n",
    "print(\"\\nCreating vectorizer...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=CONFIG['ngram_range'],\n",
    "    max_features=CONFIG['max_features'],\n",
    "    min_df=CONFIG['min_df'],\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"‚úì Vectorizer created: {len(feature_names):,} features\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=CONFIG['random_state'],\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model.fit(X_train_vec, y_train)\n",
    "print(\"‚úì Model trained\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = model.predict(X_train_vec)\n",
    "y_pred_test = model.predict(X_test_vec)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüìã Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Benign', 'Malicious']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"  TN={cm[0,0]:,}, FP={cm[0,1]:,}\")\n",
    "print(f\"  FN={cm[1,0]:,}, TP={cm[1,1]:,}\")\n",
    "\n",
    "# Save both model and vectorizer\n",
    "print(f\"\\nüíæ Saving model and vectorizer...\")\n",
    "with open('new_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('new_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print(\"‚úì Saved: new_classifier.pkl, new_vectorizer.pkl\")\n",
    "print(\"  ‚ö†Ô∏è  IMPORTANT: Always use BOTH together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 1 Analysis: Feature Importance from New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from XGBoost\n",
    "print(\"Analyzing feature importance...\\n\")\n",
    "\n",
    "# XGBoost feature importance\n",
    "importance_scores = model.feature_importances_\n",
    "\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance_scores\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 30 MOST IMPORTANT FEATURES (XGBoost Feature Importance)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nRank | Feature | Importance\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (_, row) in enumerate(feature_imp.head(30).iterrows(), 1):\n",
    "    print(f\"{i:3d}. | {row['feature']:<50} | {row['importance']:.6f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = feature_imp.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Features by XGBoost Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 2: LIME Text Explainer (Model-Agnostic)\n",
    "\n",
    "### Why LIME?\n",
    "- ‚úÖ Works directly on raw text (no vectorizer needed!)\n",
    "- ‚úÖ Model-agnostic (works with ANY classifier)\n",
    "- ‚úÖ Explains individual predictions\n",
    "- ‚úÖ Shows which WORDS matter in each prompt\n",
    "\n",
    "### How it works:\n",
    "1. Takes a text prompt\n",
    "2. Creates perturbations (removes words)\n",
    "3. Sees how predictions change\n",
    "4. Identifies important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"APPROACH 2: LIME TEXT EXPLAINER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create LIME explainer\n",
    "print(\"\\nCreating LIME explainer...\")\n",
    "\n",
    "# Prediction function for LIME\n",
    "def predict_proba_lime(texts_list):\n",
    "    \"\"\"Predict probabilities for LIME.\"\"\"\n",
    "    X_vec = vectorizer.transform(texts_list)\n",
    "    return model.predict_proba(X_vec)\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['Benign', 'Malicious'])\n",
    "print(\"‚úì LIME explainer created\")\n",
    "\n",
    "# Explain some examples\n",
    "print(\"\\nExplaining sample predictions...\\n\")\n",
    "\n",
    "# Find interesting examples\n",
    "test_indices = np.arange(len(X_test))\n",
    "mal_indices = test_indices[y_test == 1]\n",
    "ben_indices = test_indices[y_test == 0]\n",
    "\n",
    "# Correctly classified malicious\n",
    "correct_mal = mal_indices[y_pred_test[mal_indices] == 1]\n",
    "if len(correct_mal) > 0:\n",
    "    idx = correct_mal[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EXAMPLE 1: Malicious Prompt\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPrompt: {X_test[idx]}\")\n",
    "    print(f\"True: MALICIOUS, Predicted: MALICIOUS\")\n",
    "    \n",
    "    # Get LIME explanation\n",
    "    exp = explainer.explain_instance(\n",
    "        X_test[idx],\n",
    "        predict_proba_lime,\n",
    "        num_features=10,\n",
    "        num_samples=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüî• Words Contributing to MALICIOUS:\")\n",
    "    for word, weight in exp.as_list():\n",
    "        if weight > 0:\n",
    "            print(f\"  '{word:20}' ‚Üí +{weight:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüîµ Words Contributing to BENIGN:\")\n",
    "    for word, weight in exp.as_list():\n",
    "        if weight < 0:\n",
    "            print(f\"  '{word:20}' ‚Üí {weight:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    plt.title('LIME Explanation: Malicious Prompt', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correctly classified benign\n",
    "correct_ben = ben_indices[y_pred_test[ben_indices] == 0]\n",
    "if len(correct_ben) > 0:\n",
    "    idx = correct_ben[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 2: Benign Prompt\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPrompt: {X_test[idx]}\")\n",
    "    print(f\"True: BENIGN, Predicted: BENIGN\")\n",
    "    \n",
    "    exp = explainer.explain_instance(\n",
    "        X_test[idx],\n",
    "        predict_proba_lime,\n",
    "        num_features=10,\n",
    "        num_samples=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîµ Words Contributing to BENIGN:\")\n",
    "    for word, weight in exp.as_list():\n",
    "        if weight < 0:\n",
    "            print(f\"  '{word:20}' ‚Üí {weight:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüî• Words Contributing to MALICIOUS:\")\n",
    "    for word, weight in exp.as_list():\n",
    "        if weight > 0:\n",
    "            print(f\"  '{word:20}' ‚Üí +{weight:.4f}\")\n",
    "    \n",
    "    fig = exp.as_pyplot_figure()\n",
    "    plt.title('LIME Explanation: Benign Prompt', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 3: Statistical K-Gram Analysis (No Model Needed)\n",
    "\n",
    "### Why This Approach?\n",
    "- ‚úÖ No model required!\n",
    "- ‚úÖ Pure statistical analysis\n",
    "- ‚úÖ Shows which phrases appear in malicious vs benign\n",
    "- ‚úÖ Fast and simple\n",
    "\n",
    "### What it does:\n",
    "- Extracts k-grams from all prompts\n",
    "- Calculates how often each appears in malicious vs benign\n",
    "- Ranks by discriminative power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"APPROACH 3: STATISTICAL K-GRAM ANALYSIS (NO MODEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate by class\n",
    "mal_texts = [texts[i] for i in range(len(texts)) if labels[i] == 1]\n",
    "ben_texts = [texts[i] for i in range(len(texts)) if labels[i] == 0]\n",
    "\n",
    "print(f\"\\nMalicious: {len(mal_texts):,}\")\n",
    "print(f\"Benign: {len(ben_texts):,}\")\n",
    "\n",
    "# Extract k-grams from each class\n",
    "print(\"\\nExtracting k-grams...\")\n",
    "\n",
    "vec_mal = CountVectorizer(ngram_range=(1, 3), min_df=5)\n",
    "vec_ben = CountVectorizer(ngram_range=(1, 3), min_df=5)\n",
    "\n",
    "X_mal = vec_mal.fit_transform(mal_texts)\n",
    "X_ben = vec_ben.fit_transform(ben_texts)\n",
    "\n",
    "# Calculate frequencies\n",
    "mal_freq = X_mal.sum(axis=0).A1\n",
    "ben_freq = X_ben.sum(axis=0).A1\n",
    "\n",
    "# Get feature names\n",
    "mal_features = vec_mal.get_feature_names_out()\n",
    "ben_features = vec_ben.get_feature_names_out()\n",
    "\n",
    "# Find common features\n",
    "common_features = set(mal_features) & set(ben_features)\n",
    "print(f\"‚úì Found {len(common_features):,} k-grams in both classes\")\n",
    "\n",
    "# Calculate discriminative scores\n",
    "analysis = []\n",
    "for feature in common_features:\n",
    "    mal_idx = list(mal_features).index(feature)\n",
    "    ben_idx = list(ben_features).index(feature)\n",
    "    \n",
    "    mal_count = mal_freq[mal_idx]\n",
    "    ben_count = ben_freq[ben_idx]\n",
    "    \n",
    "    # Normalize by class size\n",
    "    mal_rate = mal_count / len(mal_texts)\n",
    "    ben_rate = ben_count / len(ben_texts)\n",
    "    \n",
    "    # Ratio (with smoothing)\n",
    "    ratio = (mal_rate + 0.001) / (ben_rate + 0.001)\n",
    "    \n",
    "    analysis.append({\n",
    "        'feature': feature,\n",
    "        'mal_count': int(mal_count),\n",
    "        'ben_count': int(ben_count),\n",
    "        'mal_rate': mal_rate,\n",
    "        'ben_rate': ben_rate,\n",
    "        'ratio': ratio\n",
    "    })\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis)\n",
    "\n",
    "# Most malicious (high ratio)\n",
    "most_mal = analysis_df.sort_values('ratio', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 30 MOST MALICIOUS K-GRAMS (Statistical Analysis)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nRank | K-Gram | Mal Count | Ben Count | Ratio\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (_, row) in enumerate(most_mal.head(30).iterrows(), 1):\n",
    "    print(f\"{i:3d}. | {row['feature']:<40} | {row['mal_count']:6d} | \"\n",
    "          f\"{row['ben_count']:6d} | {row['ratio']:6.2f}x\")\n",
    "\n",
    "# Most benign (low ratio)\n",
    "most_ben = analysis_df.sort_values('ratio', ascending=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 30 MOST BENIGN K-GRAMS (Statistical Analysis)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nRank | K-Gram | Mal Count | Ben Count | Ratio\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (_, row) in enumerate(most_ben.head(30).iterrows(), 1):\n",
    "    print(f\"{i:3d}. | {row['feature']:<40} | {row['mal_count']:6d} | \"\n",
    "          f\"{row['ben_count']:6d} | {row['ratio']:6.2f}x\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY: THREE APPROACHES COMPARED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  APPROACH 1: Retrain Model + Vectorizer\")\n",
    "print(\"   ‚úÖ PROS: Perfect feature matching, can use SHAP, full control\")\n",
    "print(\"   ‚ùå CONS: Need to retrain model\")\n",
    "print(\"   üìä Results: Model trained with {:.2f}% test accuracy\".format(test_acc*100))\n",
    "print(\"   üéØ USE WHEN: You want a production-ready solution\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  APPROACH 2: LIME Text Explainer\")\n",
    "print(\"   ‚úÖ PROS: Works on raw text, model-agnostic, per-sample explanations\")\n",
    "print(\"   ‚ùå CONS: Slower, approximate explanations\")\n",
    "print(\"   üìä Results: Can explain any individual prediction\")\n",
    "print(\"   üéØ USE WHEN: You want to explain specific predictions to users\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  APPROACH 3: Statistical K-Gram Analysis\")\n",
    "print(\"   ‚úÖ PROS: No model needed, fast, simple, interpretable\")\n",
    "print(\"   ‚ùå CONS: Doesn't show how MODEL uses features\")\n",
    "print(\"   üìä Results: Pure statistical patterns in data\")\n",
    "print(\"   üéØ USE WHEN: You want to understand data patterns only\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ Use APPROACH 1 (Retrain) as your main solution\")\n",
    "print(\"   - Save both new_classifier.pkl and new_vectorizer.pkl\")\n",
    "print(\"   - Use them together for all future predictions\")\n",
    "print(\"   - Now you can use SHAP properly!\")\n",
    "print(\"\\n‚úÖ Use APPROACH 2 (LIME) for explaining predictions to users\")\n",
    "print(\"   - Shows which words triggered the detection\")\n",
    "print(\"   - Good for transparency and debugging\")\n",
    "print(\"\\n‚úÖ Use APPROACH 3 (Statistical) for quick data exploration\")\n",
    "print(\"   - Find common malicious phrases\")\n",
    "print(\"   - Create security filter rules\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export feature importance\n",
    "feature_imp.to_csv('approach1_feature_importance.csv', index=False)\n",
    "print(\"‚úì Saved: approach1_feature_importance.csv\")\n",
    "\n",
    "# Export statistical analysis\n",
    "most_mal.head(100).to_csv('approach3_malicious_kgrams.csv', index=False)\n",
    "most_ben.head(100).to_csv('approach3_benign_kgrams.csv', index=False)\n",
    "print(\"‚úì Saved: approach3_malicious_kgrams.csv\")\n",
    "print(\"‚úì Saved: approach3_benign_kgrams.csv\")\n",
    "\n",
    "# Summary\n",
    "summary = {\n",
    "    'approach1': {\n",
    "        'train_accuracy': float(train_acc),\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'features': len(feature_names),\n",
    "        'files_saved': ['new_classifier.pkl', 'new_vectorizer.pkl']\n",
    "    },\n",
    "    'approach3': {\n",
    "        'common_features': len(common_features),\n",
    "        'top_malicious_features': [\n",
    "            {'rank': i+1, 'feature': row['feature'], 'ratio': float(row['ratio'])}\n",
    "            for i, (_, row) in enumerate(most_mal.head(10).iterrows())\n",
    "        ]\n",
    "    },\n",
    "    'recommendation': 'Use Approach 1 (retrained model) with Approach 2 (LIME) for explanations'\n",
    "}\n",
    "\n",
    "with open('complete_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"‚úì Saved: complete_analysis_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
