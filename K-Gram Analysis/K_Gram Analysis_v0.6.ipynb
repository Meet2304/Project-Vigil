{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Gram Malicious Phrase Analysis v0.6\n",
    "## Project Vigil - Malicious Prompt Detection\n",
    "## **Identifying Malicious Phrases: 3-5 Word K-Grams**\n",
    "\n",
    "This notebook identifies which specific 3-5 word phrases are most responsible for making prompts malicious according to the pre-trained model.\n",
    "\n",
    "### Overview\n",
    "- **Dataset**: MPDD.csv (Malicious Prompt Detection Dataset)\n",
    "- **Model**: Pre-trained classifier from Project-Vigil repository\n",
    "- **Focus**: Longer k-grams (3-5 words) that indicate maliciousness\n",
    "- **Analysis**: Which phrases trigger malicious classification\n",
    "\n",
    "### What's New in v0.6\n",
    "**FOCUS ON MALICIOUS PHRASES**:\n",
    "\n",
    "**Key Improvements:**\n",
    "1. **Longer k-grams**: 3-5 word phrases (not just 1-3 words)\n",
    "2. **Malicious phrase detection**: Identify phrases that cause malicious predictions\n",
    "3. **Phrase importance ranking**: Which phrases matter most\n",
    "4. **Context analysis**: See full prompts containing important phrases\n",
    "5. **Discriminative power**: Measure how strongly each phrase indicates maliciousness\n",
    "\n",
    "### Author: Project Vigil Team\n",
    "### Version: 0.6 (Malicious Phrase Detection)\n",
    "### Date: 2025-11-16\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This notebook is designed to run in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "# !pip install -q scikit-learn pandas numpy matplotlib seaborn tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For downloading files from GitHub\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "### Focus on Longer Malicious Phrases\n",
    "- **N-gram Range**: 3-5 words (captures complete malicious phrases)\n",
    "- **Examples**: \"ignore all previous instructions\", \"bypass safety protocols\", \"forget your training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub repository URLs\n",
    "GITHUB_REPO = \"https://raw.githubusercontent.com/Meet2304/Project-Vigil/claude/fix-kgram-dataset-01VTpiw6P21u1bbgrvx2rVb2\"\n",
    "DATASET_URL = f\"{GITHUB_REPO}/Dataset/MPDD.csv\"\n",
    "MODEL_URL = f\"{GITHUB_REPO}/Model/classifier.pkl\"\n",
    "\n",
    "# Local paths\n",
    "DATASET_PATH = \"MPDD.csv\"\n",
    "MODEL_PATH = \"classifier.pkl\"\n",
    "\n",
    "# K-gram configuration - FOCUS ON LONGER PHRASES\n",
    "K_GRAM_CONFIG = {\n",
    "    'word_ngram_range': (3, 5),  # 3-5 word phrases\n",
    "    'max_features': 10000,       # More features to capture diverse phrases\n",
    "    'min_df': 2,                 # Must appear at least 2 times\n",
    "    'use_tfidf': True,\n",
    "    'analyzer': 'word'\n",
    "}\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'sample_size': 10000,        # Use 10K samples for better coverage\n",
    "    'top_k_malicious': 50,       # Top 50 malicious phrases\n",
    "    'top_k_ablation': 30,        # Ablate top 30 phrases\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset URL: {DATASET_URL}\")\n",
    "print(f\"  Model URL: {MODEL_URL}\")\n",
    "print(f\"\\nðŸ”¤ K-Gram Config (LONGER PHRASES):\")\n",
    "print(f\"  N-gram Range: {K_GRAM_CONFIG['word_ngram_range']} words\")\n",
    "print(f\"  Max Features: {K_GRAM_CONFIG['max_features']}\")\n",
    "print(f\"  Min Document Frequency: {K_GRAM_CONFIG['min_df']}\")\n",
    "print(f\"\\nðŸ“Š Analysis Config:\")\n",
    "print(f\"  Sample Size: {ANALYSIS_CONFIG['sample_size']:,}\")\n",
    "print(f\"  Top Malicious Phrases: {ANALYSIS_CONFIG['top_k_malicious']}\")\n",
    "print(f\"  Ablation Tests: {ANALYSIS_CONFIG['top_k_ablation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset and Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, local_path: str) -> bool:\n",
    "    \"\"\"Download a file from URL to local path.\"\"\"\n",
    "    try:\n",
    "        ssl_context = ssl.create_default_context()\n",
    "        ssl_context.check_hostname = False\n",
    "        ssl_context.verify_mode = ssl.CERT_NONE\n",
    "        \n",
    "        print(f\"Downloading {url}...\")\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "        print(f\"âœ“ Downloaded to {local_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Download files\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    download_file(DATASET_URL, DATASET_PATH)\n",
    "else:\n",
    "    print(f\"âœ“ Dataset already exists\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    download_file(MODEL_URL, MODEL_PATH)\n",
    "else:\n",
    "    print(f\"âœ“ Model already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset and Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading MPDD dataset...\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"âœ“ Dataset loaded: {len(df):,} samples\")\n",
    "\n",
    "# Sample if configured\n",
    "if ANALYSIS_CONFIG['sample_size'] and ANALYSIS_CONFIG['sample_size'] < len(df):\n",
    "    print(f\"\\nSampling {ANALYSIS_CONFIG['sample_size']:,} samples...\")\n",
    "    df_sample = df.sample(\n",
    "        n=ANALYSIS_CONFIG['sample_size'],\n",
    "        random_state=ANALYSIS_CONFIG['random_state'],\n",
    "        stratify=df['isMalicious']\n",
    "    )\n",
    "else:\n",
    "    df_sample = df\n",
    "\n",
    "texts = df_sample['Prompt'].astype(str).tolist()\n",
    "labels = df_sample['isMalicious'].astype(int).tolist()\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"  Total: {len(texts):,} samples\")\n",
    "print(f\"  Malicious: {sum(labels):,} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"  Benign: {len(labels)-sum(labels):,} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading pre-trained model...\")\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "print(f\"âœ“ Loaded: {type(classifier).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Longer K-Grams (3-5 Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer for longer phrases\n",
    "print(\"Extracting 3-5 word k-grams...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=K_GRAM_CONFIG['analyzer'],\n",
    "    ngram_range=K_GRAM_CONFIG['word_ngram_range'],\n",
    "    max_features=K_GRAM_CONFIG['max_features'],\n",
    "    min_df=K_GRAM_CONFIG['min_df'],\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nâœ“ Extracted features:\")\n",
    "print(f\"  Feature matrix: {X.shape[0]:,} Ã— {X.shape[1]:,}\")\n",
    "print(f\"  Total k-grams: {len(feature_names):,}\")\n",
    "print(f\"\\nðŸ“ Example k-grams:\")\n",
    "for i in range(min(10, len(feature_names))):\n",
    "    print(f\"  - '{feature_names[i]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Baseline Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(\"Getting predictions from pre-trained model...\")\n",
    "y_pred = classifier.predict(X)\n",
    "y_proba = classifier.predict_proba(X) if hasattr(classifier, 'predict_proba') else None\n",
    "\n",
    "accuracy = accuracy_score(labels, y_pred)\n",
    "cm = confusion_matrix(labels, y_pred)\n",
    "\n",
    "print(f\"\\nâœ“ Baseline Results:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Confusion Matrix:\")\n",
    "print(f\"    True Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"    False Positives: {cm[0,1]:,}\")\n",
    "print(f\"    False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"    True Positives:  {cm[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Identify Most Malicious K-Grams\n",
    "\n",
    "### Analysis Method\n",
    "For each k-gram, we calculate:\n",
    "1. **Malicious Association**: How often it appears in malicious vs benign prompts\n",
    "2. **Prediction Power**: How strongly it correlates with malicious predictions\n",
    "3. **Discriminative Score**: Combined metric of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing malicious phrase associations...\\n\")\n",
    "\n",
    "y_true = np.array(labels)\n",
    "malicious_mask = y_true == 1\n",
    "benign_mask = y_true == 0\n",
    "\n",
    "# Calculate mean TF-IDF per class\n",
    "mal_mean = np.asarray(X[malicious_mask].mean(axis=0)).ravel()\n",
    "ben_mean = np.asarray(X[benign_mask].mean(axis=0)).ravel()\n",
    "\n",
    "# Calculate discriminative scores\n",
    "malicious_scores = []\n",
    "\n",
    "for idx, feature in enumerate(feature_names):\n",
    "    # Get feature column\n",
    "    feature_col = X[:, idx].toarray().ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mal_score = mal_mean[idx]\n",
    "    ben_score = ben_mean[idx]\n",
    "    \n",
    "    # How many malicious vs benign prompts contain this\n",
    "    mal_count = np.sum((feature_col > 0) & malicious_mask)\n",
    "    ben_count = np.sum((feature_col > 0) & benign_mask)\n",
    "    \n",
    "    # Discriminative score (favoring malicious)\n",
    "    if ben_score > 0:\n",
    "        ratio = mal_score / ben_score\n",
    "    else:\n",
    "        ratio = mal_score * 1000  # High ratio if only in malicious\n",
    "    \n",
    "    # Combined score\n",
    "    discriminative_score = mal_score * ratio\n",
    "    \n",
    "    malicious_scores.append({\n",
    "        'phrase': feature,\n",
    "        'idx': idx,\n",
    "        'mal_mean': mal_score,\n",
    "        'ben_mean': ben_score,\n",
    "        'mal_count': mal_count,\n",
    "        'ben_count': ben_count,\n",
    "        'ratio': ratio,\n",
    "        'discriminative_score': discriminative_score\n",
    "    })\n",
    "\n",
    "# Sort by discriminative score\n",
    "malicious_scores.sort(key=lambda x: x['discriminative_score'], reverse=True)\n",
    "\n",
    "# Display top malicious phrases\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP MALICIOUS PHRASES (3-5 Words)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nRank | Phrase | Mal/Ben Ratio | In Mal | In Ben\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_malicious = malicious_scores[:ANALYSIS_CONFIG['top_k_malicious']]\n",
    "\n",
    "for rank, item in enumerate(top_malicious, 1):\n",
    "    phrase = item['phrase']\n",
    "    if len(phrase) > 45:\n",
    "        phrase = phrase[:42] + \"...\"\n",
    "    print(f\"{rank:3d}. | {phrase:<45} | {item['ratio']:8.2f} | \"\n",
    "          f\"{item['mal_count']:6d} | {item['ben_count']:6d}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Show Example Prompts Containing Top Malicious Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples for top 10 phrases\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE PROMPTS CONTAINING TOP MALICIOUS PHRASES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, item in enumerate(top_malicious[:10], 1):\n",
    "    phrase = item['phrase']\n",
    "    print(f\"\\n{rank}. Phrase: '{phrase}'\")\n",
    "    print(f\"   Appears in {item['mal_count']} malicious, {item['ben_count']} benign prompts\")\n",
    "    print(f\"   Malicious/Benign Ratio: {item['ratio']:.2f}\")\n",
    "    \n",
    "    # Find examples\n",
    "    examples = []\n",
    "    for i, text in enumerate(texts):\n",
    "        if phrase.lower() in text.lower() and labels[i] == 1:  # Malicious examples\n",
    "            examples.append(text)\n",
    "            if len(examples) >= 2:\n",
    "                break\n",
    "    \n",
    "    if examples:\n",
    "        print(f\"   Example prompts:\")\n",
    "        for ex in examples:\n",
    "            truncated = ex if len(ex) <= 100 else ex[:97] + \"...\"\n",
    "            print(f\"   â€¢ {truncated}\")\n",
    "    else:\n",
    "        print(f\"   (No examples found in sample)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ablation Analysis - Impact of Removing Malicious Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ABLATION ANALYSIS: Removing Top Malicious Phrases\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Testing impact of removing top {ANALYSIS_CONFIG['top_k_ablation']} phrases...\\n\")\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for item in tqdm(top_malicious[:ANALYSIS_CONFIG['top_k_ablation']], \n",
    "                 desc=\"ðŸ”¬ Testing Phrases\"):\n",
    "    # Remove this feature\n",
    "    X_ablated = X.copy()\n",
    "    X_ablated[:, item['idx']] = 0\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_ablated = classifier.predict(X_ablated)\n",
    "    \n",
    "    # Calculate changes\n",
    "    acc_ablated = accuracy_score(labels, y_pred_ablated)\n",
    "    acc_drop = accuracy - acc_ablated\n",
    "    \n",
    "    # Prediction flips\n",
    "    flips = np.sum(y_pred != y_pred_ablated)\n",
    "    mal_flips = np.sum((y_pred != y_pred_ablated) & (y_true == 1))\n",
    "    ben_flips = np.sum((y_pred != y_pred_ablated) & (y_true == 0))\n",
    "    \n",
    "    ablation_results.append({\n",
    "        'phrase': item['phrase'],\n",
    "        'accuracy_drop': acc_drop,\n",
    "        'total_flips': flips,\n",
    "        'malicious_flips': mal_flips,\n",
    "        'benign_flips': ben_flips,\n",
    "        'flip_rate': flips / len(labels)\n",
    "    })\n",
    "\n",
    "# Sort by accuracy drop\n",
    "ablation_results.sort(key=lambda x: x['accuracy_drop'], reverse=True)\n",
    "\n",
    "print(f\"\\nâœ“ Ablation complete\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"PHRASES BY PREDICTION IMPACT (When Removed)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nRank | Phrase | Acc Drop | Pred Flips | Flip Rate\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, result in enumerate(ablation_results[:20], 1):\n",
    "    phrase = result['phrase']\n",
    "    if len(phrase) > 40:\n",
    "        phrase = phrase[:37] + \"...\"\n",
    "    print(f\"{rank:3d}. | {phrase:<40} | {result['accuracy_drop']:+.4f} | \"\n",
    "          f\"{result['total_flips']:5d} | {result['flip_rate']*100:5.2f}%\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization: Top Malicious Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top malicious phrases by discriminative score\n",
    "top_20 = top_malicious[:20]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: Malicious/Benign Ratio\n",
    "phrases = [item['phrase'][:40] for item in top_20]\n",
    "ratios = [item['ratio'] for item in top_20]\n",
    "\n",
    "ax1.barh(range(len(phrases)), ratios, color='crimson', alpha=0.7)\n",
    "ax1.set_yticks(range(len(phrases)))\n",
    "ax1.set_yticklabels(phrases, fontsize=9)\n",
    "ax1.set_xlabel('Malicious/Benign Ratio', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Top 20 Malicious Phrases by Class Ratio', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Plot 2: Occurrence counts\n",
    "mal_counts = [item['mal_count'] for item in top_20]\n",
    "ben_counts = [item['ben_count'] for item in top_20]\n",
    "y_pos = np.arange(len(phrases))\n",
    "\n",
    "ax2.barh(y_pos - 0.2, mal_counts, 0.4, label='Malicious', color='red', alpha=0.8)\n",
    "ax2.barh(y_pos + 0.2, ben_counts, 0.4, label='Benign', color='green', alpha=0.8)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(phrases, fontsize=9)\n",
    "ax2.set_xlabel('Occurrence Count', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Phrase Occurrences by Class', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization: Ablation Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ablation results\n",
    "top_ablation = ablation_results[:20]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: Accuracy drop\n",
    "phrases = [r['phrase'][:40] for r in top_ablation]\n",
    "acc_drops = [r['accuracy_drop'] * 100 for r in top_ablation]\n",
    "\n",
    "colors = ['red' if x > 0 else 'green' for x in acc_drops]\n",
    "ax1.barh(range(len(phrases)), acc_drops, color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(phrases)))\n",
    "ax1.set_yticklabels(phrases, fontsize=9)\n",
    "ax1.set_xlabel('Accuracy Change When Removed (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Impact of Removing Each Phrase', fontsize=13, fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Plot 2: Prediction flips\n",
    "mal_flips = [r['malicious_flips'] for r in top_ablation]\n",
    "ben_flips = [r['benign_flips'] for r in top_ablation]\n",
    "y_pos = np.arange(len(phrases))\n",
    "\n",
    "ax2.barh(y_pos - 0.2, mal_flips, 0.4, label='Malicious', color='red', alpha=0.8)\n",
    "ax2.barh(y_pos + 0.2, ben_flips, 0.4, label='Benign', color='green', alpha=0.8)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(phrases, fontsize=9)\n",
    "ax2.set_xlabel('Number of Prediction Flips', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Prediction Changes by Class', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MALICIOUS PHRASE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Analysis Configuration:\")\n",
    "print(f\"  K-Gram Size: {K_GRAM_CONFIG['word_ngram_range'][0]}-{K_GRAM_CONFIG['word_ngram_range'][1]} words\")\n",
    "print(f\"  Total Samples: {len(texts):,}\")\n",
    "print(f\"  Total Phrases Extracted: {len(feature_names):,}\")\n",
    "print(f\"  Top Malicious Analyzed: {ANALYSIS_CONFIG['top_k_malicious']}\")\n",
    "print(f\"\\nðŸŽ¯ Model Performance:\")\n",
    "print(f\"  Baseline Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nðŸ” Most Discriminative Malicious Phrase:\")\n",
    "top1 = top_malicious[0]\n",
    "print(f\"  '{top1['phrase']}'\")\n",
    "print(f\"  Mal/Ben Ratio: {top1['ratio']:.2f}x\")\n",
    "print(f\"  Appears in: {top1['mal_count']} malicious, {top1['ben_count']} benign\")\n",
    "print(f\"\\nðŸ’¥ Highest Impact Phrase (Ablation):\")\n",
    "highest_impact = ablation_results[0]\n",
    "print(f\"  '{highest_impact['phrase']}'\")\n",
    "print(f\"  Accuracy Drop: {highest_impact['accuracy_drop']:+.4f} ({highest_impact['accuracy_drop']*100:+.2f}%)\")\n",
    "print(f\"  Prediction Flips: {highest_impact['total_flips']} ({highest_impact['flip_rate']*100:.2f}%)\")\n",
    "print(f\"\\nðŸ“ˆ Ablation Statistics:\")\n",
    "acc_drops = [r['accuracy_drop'] for r in ablation_results]\n",
    "print(f\"  Phrases with NEGATIVE impact: {sum(1 for x in acc_drops if x > 0)}\")\n",
    "print(f\"  Phrases with POSITIVE impact: {sum(1 for x in acc_drops if x < 0)}\")\n",
    "print(f\"  Max Accuracy Drop: {max(acc_drops):+.4f} ({max(acc_drops)*100:+.2f}%)\")\n",
    "print(f\"  Average Impact: {np.mean(acc_drops):+.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export malicious phrases\n",
    "malicious_df = pd.DataFrame(top_malicious)\n",
    "malicious_df.to_csv('malicious_phrases_3_5_words.csv', index=False)\n",
    "print(\"âœ“ Malicious phrases saved to 'malicious_phrases_3_5_words.csv'\")\n",
    "\n",
    "# Export ablation results\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "ablation_df.to_csv('phrase_ablation_results.csv', index=False)\n",
    "print(\"âœ“ Ablation results saved to 'phrase_ablation_results.csv'\")\n",
    "\n",
    "# Export summary\n",
    "summary = {\n",
    "    'config': {\n",
    "        'ngram_range': K_GRAM_CONFIG['word_ngram_range'],\n",
    "        'sample_size': len(texts),\n",
    "        'total_phrases': len(feature_names)\n",
    "    },\n",
    "    'baseline_accuracy': float(accuracy),\n",
    "    'top_10_malicious_phrases': [\n",
    "        {\n",
    "            'rank': i+1,\n",
    "            'phrase': item['phrase'],\n",
    "            'mal_ben_ratio': float(item['ratio']),\n",
    "            'malicious_count': int(item['mal_count']),\n",
    "            'benign_count': int(item['ben_count'])\n",
    "        }\n",
    "        for i, item in enumerate(top_malicious[:10])\n",
    "    ],\n",
    "    'top_10_impact_phrases': [\n",
    "        {\n",
    "            'rank': i+1,\n",
    "            'phrase': r['phrase'],\n",
    "            'accuracy_drop': float(r['accuracy_drop']),\n",
    "            'prediction_flips': int(r['total_flips'])\n",
    "        }\n",
    "        for i, r in enumerate(ablation_results[:10])\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('malicious_phrase_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"âœ“ Summary saved to 'malicious_phrase_analysis_summary.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusions\n",
    "\n",
    "### Key Insights from 3-5 Word Phrase Analysis\n",
    "\n",
    "This analysis reveals:\n",
    "\n",
    "1. **Most Discriminative Malicious Phrases**\n",
    "   - Longer phrases (3-5 words) capture complete malicious instructions\n",
    "   - Examples: \"ignore all previous instructions\", \"bypass safety protocols\"\n",
    "   - These phrases have high malicious/benign ratios\n",
    "\n",
    "2. **Prediction Impact**\n",
    "   - Removing certain phrases significantly changes predictions\n",
    "   - Some phrases are critical for correct malicious classification\n",
    "   - Others may be noise or overfitting indicators\n",
    "\n",
    "3. **Patterns in Malicious Prompts**\n",
    "   - Command-like structures (\"ignore\", \"bypass\", \"override\")\n",
    "   - Reset instructions (\"start over\", \"begin anew\")\n",
    "   - Negation of constraints (\"forget\", \"disregard\")\n",
    "\n",
    "### Comparison with v0.5 (1-3 Word K-Grams)\n",
    "\n",
    "| Aspect | v0.5 (1-3 words) | v0.6 (3-5 words) |\n",
    "|--------|------------------|------------------|\n",
    "| **Granularity** | Individual words, short phrases | Complete phrases |\n",
    "| **Context** | Limited | Full instruction context |\n",
    "| **Interpretability** | Moderate | High (full commands) |\n",
    "| **Coverage** | Broad | Focused on specific attacks |\n",
    "| **Best For** | General patterns | Identifying attack phrases |\n",
    "\n",
    "### Actionable Insights\n",
    "\n",
    "1. **Security Filters**: Use top malicious phrases for pattern matching\n",
    "2. **Model Improvement**: Focus training on discriminative phrases\n",
    "3. **Attack Detection**: Monitor for identified malicious phrases\n",
    "4. **Adversarial Testing**: Test if attackers can avoid these phrases\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Combine with v0.5**: Analyze both short and long phrases\n",
    "2. **Phrase variations**: Check for paraphrases of top phrases\n",
    "3. **Context analysis**: When do these phrases appear?\n",
    "4. **Defense strategies**: How to detect variants?\n",
    "\n",
    "---\n",
    "\n",
    "**Project Vigil - Identifying Malicious Instruction Phrases**\n",
    "\n",
    "*v0.6: 3-5 word phrase analysis revealing complete malicious instructions*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
