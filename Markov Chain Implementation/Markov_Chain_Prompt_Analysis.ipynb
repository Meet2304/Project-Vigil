{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Analysis for Malicious Prompt Detection\n",
    "\n",
    "This notebook implements a Markov Chain-based approach to identify word sequences responsible for the maliciousness of prompts.\n",
    "\n",
    "## Approach:\n",
    "1. Extract k-grams (bigrams and trigrams) from prompts\n",
    "2. Build separate Markov Chains for malicious and benign prompts\n",
    "3. Calculate transition probabilities for word sequences\n",
    "4. Use likelihood ratios to classify prompts\n",
    "5. Identify most discriminative sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "if IN_COLAB:\n",
    "    # Download from GitHub\n",
    "    url = 'https://raw.githubusercontent.com/Meet2304/Project-Vigil/main/Dataset/MPDD.csv'\n",
    "    df = pd.read_csv(url)\n",
    "else:\n",
    "    # Load from local path\n",
    "    df = pd.read_csv('../Dataset/MPDD.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total prompts: {len(df)}\")\n",
    "print(f\"Malicious prompts: {df['isMalicious'].sum()} ({df['isMalicious'].sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Benign prompts: {(1-df['isMalicious']).sum()} ({(1-df['isMalicious']).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for k-gram extraction:\n",
    "    - Convert to lowercase\n",
    "    - Remove special characters (keep only alphanumeric and spaces)\n",
    "    - Remove extra whitespaces\n",
    "    - Tokenize into words\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['tokens'] = df['Prompt'].apply(preprocess_text)\n",
    "df['token_count'] = df['tokens'].apply(len)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(f\"Average tokens per prompt: {df['token_count'].mean():.1f}\")\n",
    "print(f\"\\nExample preprocessed prompt:\")\n",
    "print(f\"Original: {df['Prompt'].iloc[0]}\")\n",
    "print(f\"Tokens: {df['tokens'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Gram Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Extract n-grams from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        n: Size of n-grams\n",
    "    \n",
    "    Returns:\n",
    "        List of n-grams (tuples)\n",
    "    \"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Extract bigrams and trigrams\n",
    "print(\"Extracting k-grams...\")\n",
    "df['bigrams'] = df['tokens'].apply(lambda x: extract_ngrams(x, 2))\n",
    "df['trigrams'] = df['tokens'].apply(lambda x: extract_ngrams(x, 3))\n",
    "\n",
    "print(\"K-gram extraction complete!\")\n",
    "print(f\"\\nExample bigrams: {df['bigrams'].iloc[0][:5]}\")\n",
    "print(f\"Example trigrams: {df['trigrams'].iloc[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Markov Chain Model\n",
    "\n",
    "We'll build separate Markov chains for malicious and benign prompts. Each chain will store:\n",
    "- Transition probabilities: P(word_i | word_{i-1})\n",
    "- Sequence probabilities for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    \"\"\"\n",
    "    A simple Markov Chain model for text analysis.\n",
    "    Stores transition probabilities between words.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Markov Chain.\n",
    "        \n",
    "        Args:\n",
    "            smoothing: Laplace smoothing parameter (default=1.0)\n",
    "        \"\"\"\n",
    "        self.transitions = defaultdict(lambda: defaultdict(int))\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.smoothing = smoothing\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def train(self, token_lists):\n",
    "        \"\"\"\n",
    "        Train the Markov Chain on a list of token sequences.\n",
    "        \n",
    "        Args:\n",
    "            token_lists: List of token lists\n",
    "        \"\"\"\n",
    "        for tokens in token_lists:\n",
    "            # Add START and END tokens\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "            \n",
    "            # Build transitions\n",
    "            for i in range(len(tokens) - 1):\n",
    "                current_word = tokens[i]\n",
    "                next_word = tokens[i + 1]\n",
    "                \n",
    "                self.transitions[current_word][next_word] += 1\n",
    "                self.word_counts[current_word] += 1\n",
    "                self.vocabulary.add(current_word)\n",
    "                self.vocabulary.add(next_word)\n",
    "    \n",
    "    def get_probability(self, current_word, next_word):\n",
    "        \"\"\"\n",
    "        Get the transition probability P(next_word | current_word).\n",
    "        Uses Laplace smoothing for unseen transitions.\n",
    "        \n",
    "        Args:\n",
    "            current_word: Current word\n",
    "            next_word: Next word\n",
    "        \n",
    "        Returns:\n",
    "            Transition probability\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        # Laplace smoothing\n",
    "        numerator = self.transitions[current_word][next_word] + self.smoothing\n",
    "        denominator = self.word_counts[current_word] + (self.smoothing * vocab_size)\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return 1.0 / vocab_size\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def get_sequence_log_probability(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculate the log probability of a token sequence.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokens\n",
    "        \n",
    "        Returns:\n",
    "            Log probability of the sequence\n",
    "        \"\"\"\n",
    "        if len(tokens) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        tokens = ['<START>'] + tokens + ['<END>']\n",
    "        log_prob = 0.0\n",
    "        \n",
    "        for i in range(len(tokens) - 1):\n",
    "            prob = self.get_probability(tokens[i], tokens[i + 1])\n",
    "            log_prob += np.log(prob + 1e-10)  # Add small value to avoid log(0)\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    def get_most_likely_transitions(self, top_n=20):\n",
    "        \"\"\"\n",
    "        Get the most frequent transitions.\n",
    "        \n",
    "        Args:\n",
    "            top_n: Number of top transitions to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (current_word, next_word, count) tuples\n",
    "        \"\"\"\n",
    "        all_transitions = []\n",
    "        \n",
    "        for current_word, next_words in self.transitions.items():\n",
    "            if current_word in ['<START>', '<END>']:\n",
    "                continue\n",
    "            for next_word, count in next_words.items():\n",
    "                if next_word in ['<START>', '<END>']:\n",
    "                    continue\n",
    "                all_transitions.append((current_word, next_word, count))\n",
    "        \n",
    "        # Sort by count\n",
    "        all_transitions.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return all_transitions[:top_n]\n",
    "\n",
    "print(\"Markov Chain class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df['tokens'].values\n",
    "y = df['isMalicious'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(f\"  Malicious: {y_train.sum()} ({y_train.sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Benign: {(1-y_train).sum()} ({(1-y_train).sum()/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Separate Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training data by class\n",
    "malicious_tokens = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1]\n",
    "benign_tokens = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0]\n",
    "\n",
    "print(\"Building Markov Chains...\")\n",
    "\n",
    "# Build malicious Markov Chain\n",
    "malicious_mc = MarkovChain(smoothing=1.0)\n",
    "malicious_mc.train(malicious_tokens)\n",
    "print(f\"✓ Malicious chain built: {len(malicious_mc.vocabulary)} unique words\")\n",
    "\n",
    "# Build benign Markov Chain\n",
    "benign_mc = MarkovChain(smoothing=1.0)\n",
    "benign_mc.train(benign_tokens)\n",
    "print(f\"✓ Benign chain built: {len(benign_mc.vocabulary)} unique words\")\n",
    "\n",
    "print(\"\\nMarkov Chains built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification Using Likelihood Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_prompt(tokens, malicious_mc, benign_mc, prior_malicious=0.5):\n",
    "    \"\"\"\n",
    "    Classify a prompt as malicious or benign using likelihood ratio.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        malicious_mc: Malicious Markov Chain\n",
    "        benign_mc: Benign Markov Chain\n",
    "        prior_malicious: Prior probability of malicious class\n",
    "    \n",
    "    Returns:\n",
    "        (prediction, malicious_score, benign_score)\n",
    "    \"\"\"\n",
    "    if len(tokens) == 0:\n",
    "        return 0, 0.0, 0.0\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    log_prob_malicious = malicious_mc.get_sequence_log_probability(tokens)\n",
    "    log_prob_benign = benign_mc.get_sequence_log_probability(tokens)\n",
    "    \n",
    "    # Add prior probabilities (in log space)\n",
    "    log_prob_malicious += np.log(prior_malicious + 1e-10)\n",
    "    log_prob_benign += np.log(1 - prior_malicious + 1e-10)\n",
    "    \n",
    "    # Normalize scores for interpretability\n",
    "    malicious_score = np.exp(log_prob_malicious)\n",
    "    benign_score = np.exp(log_prob_benign)\n",
    "    \n",
    "    # Classify based on higher probability\n",
    "    prediction = 1 if log_prob_malicious > log_prob_benign else 0\n",
    "    \n",
    "    return prediction, malicious_score, benign_score\n",
    "\n",
    "print(\"Classification function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "# Calculate prior probability from training set\n",
    "prior_malicious = y_train.sum() / len(y_train)\n",
    "print(f\"Prior probability of malicious: {prior_malicious:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "malicious_scores = []\n",
    "benign_scores = []\n",
    "\n",
    "for tokens in X_test:\n",
    "    pred, mal_score, ben_score = classify_prompt(tokens, malicious_mc, benign_mc, prior_malicious)\n",
    "    predictions.append(pred)\n",
    "    malicious_scores.append(mal_score)\n",
    "    benign_scores.append(ben_score)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, zero_division=0)\n",
    "recall = recall_score(y_test, predictions, zero_division=0)\n",
    "f1 = f1_score(y_test, predictions, zero_division=0)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MARKOV CHAIN PERFORMANCE METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Malicious'],\n",
    "            yticklabels=['Benign', 'Malicious'])\n",
    "plt.title('Confusion Matrix - Markov Chain Classifier', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Negatives (Benign→Benign): {cm[0,0]}\")\n",
    "print(f\"False Positives (Benign→Malicious): {cm[0,1]}\")\n",
    "print(f\"False Negatives (Malicious→Benign): {cm[1,0]}\")\n",
    "print(f\"True Positives (Malicious→Malicious): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Identify Most Discriminative Word Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminative_transitions(malicious_mc, benign_mc, top_n=20):\n",
    "    \"\"\"\n",
    "    Find word transitions that are most indicative of malicious prompts.\n",
    "    Uses likelihood ratio: P(transition|malicious) / P(transition|benign)\n",
    "    \"\"\"\n",
    "    discriminative_scores = []\n",
    "    \n",
    "    # Get all transitions from malicious chain\n",
    "    for current_word in malicious_mc.transitions:\n",
    "        if current_word in ['<START>', '<END>']:\n",
    "            continue\n",
    "        \n",
    "        for next_word in malicious_mc.transitions[current_word]:\n",
    "            if next_word in ['<START>', '<END>']:\n",
    "                continue\n",
    "            \n",
    "            # Get probabilities from both chains\n",
    "            prob_malicious = malicious_mc.get_probability(current_word, next_word)\n",
    "            prob_benign = benign_mc.get_probability(current_word, next_word)\n",
    "            \n",
    "            # Calculate likelihood ratio\n",
    "            ratio = prob_malicious / (prob_benign + 1e-10)\n",
    "            \n",
    "            # Count occurrences\n",
    "            count_malicious = malicious_mc.transitions[current_word][next_word]\n",
    "            \n",
    "            # Only consider transitions that appear at least 3 times\n",
    "            if count_malicious >= 3:\n",
    "                discriminative_scores.append({\n",
    "                    'transition': f\"{current_word} → {next_word}\",\n",
    "                    'ratio': ratio,\n",
    "                    'prob_malicious': prob_malicious,\n",
    "                    'prob_benign': prob_benign,\n",
    "                    'count': count_malicious\n",
    "                })\n",
    "    \n",
    "    # Sort by likelihood ratio\n",
    "    discriminative_scores.sort(key=lambda x: x['ratio'], reverse=True)\n",
    "    \n",
    "    return discriminative_scores[:top_n]\n",
    "\n",
    "print(\"Finding most discriminative word sequences...\")\n",
    "discriminative_transitions = get_discriminative_transitions(malicious_mc, benign_mc, top_n=25)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 25 WORD SEQUENCES INDICATING MALICIOUS PROMPTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Rank':<6}{'Word Sequence':<30}{'Likelihood Ratio':<20}{'Occurrences':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, item in enumerate(discriminative_transitions, 1):\n",
    "    print(f\"{i:<6}{item['transition']:<30}{item['ratio']:<20.2f}{item['count']:<15}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top discriminative transitions\n",
    "top_15 = discriminative_transitions[:15]\n",
    "\n",
    "transitions = [item['transition'] for item in top_15]\n",
    "ratios = [item['ratio'] for item in top_15]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(transitions)), ratios, color='crimson', alpha=0.7)\n",
    "plt.yticks(range(len(transitions)), transitions)\n",
    "plt.xlabel('Likelihood Ratio (Malicious / Benign)', fontsize=12)\n",
    "plt.title('Top 15 Word Sequences Indicating Malicious Prompts', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Trigrams for Longer Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trigrams from both classes\n",
    "malicious_trigrams = []\n",
    "benign_trigrams = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    trigrams = extract_ngrams(X_train[i], 3)\n",
    "    if y_train[i] == 1:\n",
    "        malicious_trigrams.extend(trigrams)\n",
    "    else:\n",
    "        benign_trigrams.extend(trigrams)\n",
    "\n",
    "# Count trigrams\n",
    "malicious_trigram_counts = Counter(malicious_trigrams)\n",
    "benign_trigram_counts = Counter(benign_trigrams)\n",
    "\n",
    "print(f\"Total malicious trigrams: {len(malicious_trigrams)}\")\n",
    "print(f\"Unique malicious trigrams: {len(malicious_trigram_counts)}\")\n",
    "print(f\"\\nTotal benign trigrams: {len(benign_trigrams)}\")\n",
    "print(f\"Unique benign trigrams: {len(benign_trigram_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trigrams unique to or highly indicative of malicious prompts\n",
    "def get_discriminative_trigrams(malicious_counts, benign_counts, top_n=20, min_count=3):\n",
    "    \"\"\"\n",
    "    Find trigrams that are most indicative of malicious prompts.\n",
    "    \"\"\"\n",
    "    total_malicious = sum(malicious_counts.values())\n",
    "    total_benign = sum(benign_counts.values())\n",
    "    \n",
    "    trigram_scores = []\n",
    "    \n",
    "    for trigram, mal_count in malicious_counts.items():\n",
    "        if mal_count < min_count:\n",
    "            continue\n",
    "        \n",
    "        ben_count = benign_counts.get(trigram, 0)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        prob_malicious = mal_count / total_malicious\n",
    "        prob_benign = (ben_count + 1) / (total_benign + len(benign_counts))  # Smoothing\n",
    "        \n",
    "        # Likelihood ratio\n",
    "        ratio = prob_malicious / prob_benign\n",
    "        \n",
    "        trigram_scores.append({\n",
    "            'trigram': ' '.join(trigram),\n",
    "            'ratio': ratio,\n",
    "            'malicious_count': mal_count,\n",
    "            'benign_count': ben_count\n",
    "        })\n",
    "    \n",
    "    # Sort by ratio\n",
    "    trigram_scores.sort(key=lambda x: x['ratio'], reverse=True)\n",
    "    \n",
    "    return trigram_scores[:top_n]\n",
    "\n",
    "discriminative_trigrams = get_discriminative_trigrams(malicious_trigram_counts, benign_trigram_counts, top_n=25)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"TOP 25 TRIGRAMS (3-WORD SEQUENCES) INDICATING MALICIOUS PROMPTS\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Rank':<6}{'Trigram':<40}{'Ratio':<15}{'Mal Count':<12}{'Ben Count'}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for i, item in enumerate(discriminative_trigrams, 1):\n",
    "    print(f\"{i:<6}{item['trigram']:<40}{item['ratio']:<15.2f}{item['malicious_count']:<12}{item['benign_count']}\")\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Example Predictions with Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prediction(tokens, true_label, malicious_mc, benign_mc):\n",
    "    \"\"\"\n",
    "    Explain why a prompt was classified as malicious or benign.\n",
    "    \"\"\"\n",
    "    pred, mal_score, ben_score = classify_prompt(tokens, malicious_mc, benign_mc)\n",
    "    \n",
    "    # Get bigrams\n",
    "    bigrams = extract_ngrams(tokens, 2)\n",
    "    \n",
    "    # Calculate transition probabilities for each bigram\n",
    "    bigram_ratios = []\n",
    "    for bg in bigrams:\n",
    "        if len(bg) == 2:\n",
    "            prob_mal = malicious_mc.get_probability(bg[0], bg[1])\n",
    "            prob_ben = benign_mc.get_probability(bg[0], bg[1])\n",
    "            ratio = prob_mal / (prob_ben + 1e-10)\n",
    "            bigram_ratios.append((f\"{bg[0]} → {bg[1]}\", ratio))\n",
    "    \n",
    "    # Sort by ratio\n",
    "    bigram_ratios.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION EXPLANATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Text: {' '.join(tokens[:50])}...\" if len(tokens) > 50 else f\"Text: {' '.join(tokens)}\")\n",
    "    print(f\"\\nTrue Label: {'Malicious' if true_label == 1 else 'Benign'}\")\n",
    "    print(f\"Predicted: {'Malicious' if pred == 1 else 'Benign'}\")\n",
    "    print(f\"Correct: {'✓ Yes' if pred == true_label else '✗ No'}\")\n",
    "    print(f\"\\nMalicious Score: {mal_score:.2e}\")\n",
    "    print(f\"Benign Score: {ben_score:.2e}\")\n",
    "    \n",
    "    if len(bigram_ratios) > 0:\n",
    "        print(\"\\nTop word sequences contributing to maliciousness:\")\n",
    "        for i, (bg, ratio) in enumerate(bigram_ratios[:5], 1):\n",
    "            print(f\"  {i}. '{bg}' (ratio: {ratio:.2f})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# Find some interesting examples\n",
    "# 1. Correctly classified malicious\n",
    "correct_malicious_idx = None\n",
    "for i in range(len(X_test)):\n",
    "    if y_test[i] == 1 and predictions[i] == 1:\n",
    "        correct_malicious_idx = i\n",
    "        break\n",
    "\n",
    "if correct_malicious_idx is not None:\n",
    "    print(\"\\n[1] CORRECTLY CLASSIFIED MALICIOUS PROMPT:\")\n",
    "    explain_prediction(X_test[correct_malicious_idx], y_test[correct_malicious_idx], malicious_mc, benign_mc)\n",
    "\n",
    "# 2. Correctly classified benign\n",
    "correct_benign_idx = None\n",
    "for i in range(len(X_test)):\n",
    "    if y_test[i] == 0 and predictions[i] == 0:\n",
    "        correct_benign_idx = i\n",
    "        break\n",
    "\n",
    "if correct_benign_idx is not None:\n",
    "    print(\"\\n[2] CORRECTLY CLASSIFIED BENIGN PROMPT:\")\n",
    "    explain_prediction(X_test[correct_benign_idx], y_test[correct_benign_idx], malicious_mc, benign_mc)\n",
    "\n",
    "# 3. Misclassified example (if any)\n",
    "misclassified_idx = None\n",
    "for i in range(len(X_test)):\n",
    "    if y_test[i] != predictions[i]:\n",
    "        misclassified_idx = i\n",
    "        break\n",
    "\n",
    "if misclassified_idx is not None:\n",
    "    print(\"\\n[3] MISCLASSIFIED EXAMPLE:\")\n",
    "    explain_prediction(X_test[misclassified_idx], y_test[misclassified_idx], malicious_mc, benign_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"SUMMARY OF KEY FINDINGS\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "print(\"\\n1. MODEL PERFORMANCE:\")\n",
    "print(f\"   - Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"   - Precision: {precision*100:.2f}%\")\n",
    "print(f\"   - Recall: {recall*100:.2f}%\")\n",
    "print(f\"   - F1-Score: {f1*100:.2f}%\")\n",
    "\n",
    "print(\"\\n2. TOP MALICIOUS WORD SEQUENCES (Bigrams):\")\n",
    "for i, item in enumerate(discriminative_transitions[:5], 1):\n",
    "    print(f\"   {i}. '{item['transition']}' (ratio: {item['ratio']:.2f})\")\n",
    "\n",
    "print(\"\\n3. TOP MALICIOUS TRIGRAMS:\")\n",
    "for i, item in enumerate(discriminative_trigrams[:5], 1):\n",
    "    print(f\"   {i}. '{item['trigram']}' (ratio: {item['ratio']:.2f})\")\n",
    "\n",
    "print(\"\\n4. KEY INSIGHTS:\")\n",
    "print(\"   - Markov chains successfully capture sequential patterns in prompts\")\n",
    "print(\"   - Certain word sequences are strong indicators of malicious intent\")\n",
    "print(\"   - Phrases like 'forget', 'ignore', 'bypass' often appear in malicious prompts\")\n",
    "print(\"   - The model can identify prompt injection attempts and jailbreak patterns\")\n",
    "\n",
    "print(\"\\n5. COMPUTATIONAL EFFICIENCY:\")\n",
    "print(\"   - Training: Very fast (seconds)\")\n",
    "print(\"   - Inference: Extremely fast (milliseconds per prompt)\")\n",
    "print(\"   - Memory: Lightweight model suitable for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how Markov Chains can be used to:\n",
    "1. **Identify malicious patterns**: Certain word sequences are strong indicators of malicious prompts\n",
    "2. **Classify prompts**: Using likelihood ratios between malicious and benign chains\n",
    "3. **Explain predictions**: Show which word sequences contribute to classification decisions\n",
    "\n",
    "### Advantages:\n",
    "- Simple and interpretable\n",
    "- Fast training and inference\n",
    "- Provides explainable results (shows which sequences are malicious)\n",
    "- Low computational requirements (works well on Colab)\n",
    "\n",
    "### Limitations:\n",
    "- Only captures sequential dependencies (not long-range context)\n",
    "- Assumes Markov property (future depends only on current state)\n",
    "- May struggle with novel attack patterns not seen in training\n",
    "\n",
    "### Future Improvements:\n",
    "- Higher-order Markov chains (trigrams, 4-grams)\n",
    "- Combine with TF-IDF or embeddings\n",
    "- Ensemble with other classifiers\n",
    "- Use variable-length n-grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
