# K-Gram Analysis: Complete Workflow Analysis & Improvements

## ğŸ“‹ Executive Summary

This document analyzes the k-gram analysis approaches (v0.5, v0.6) and presents an improved workflow (v0.7) using SHAP for proper feature attribution.

---

## ğŸ” Analysis of Current Results (v0.6)

### Issues Identified

#### 1. **Model Classifies Everything as Malicious**
```
Confusion Matrix:
  TN=0, FP=5000, FN=0, TP=5000
Accuracy: 50% (random)
```

**Problem**: Model predicts malicious for ALL samples, even benign ones.

**Root Causes**:
- Vectorizer mismatch (created new vectorizer instead of using model's original)
- Model expects specific features from training, but receives different ones
- Like speaking English to someone expecting French!

#### 2. **Ablation Shows Zero Impact**
```
Removing ANY phrase:
  Accuracy Drop: 0.00%
  Prediction Flips: 0
```

**Problem**: Removing even the most "malicious" phrases has ZERO effect.

**Root Cause**: Since all predictions are the same (malicious), removing features can't change anything!

#### 3. **Phrases Identified Don't Make Sense**
```
Top phrases:
- "and begin afresh"
- "science and technology"
- "this piece of news regarding world politics"
```

**Problem**: Some seem malicious, others don't.

**Root Cause**: High Mal/Ben ratio just means they appear in malicious samples, not that the MODEL uses them.

---

## ğŸ¯ Root Cause Analysis

### The Fundamental Issue: **Vectorizer Mismatch**

```python
# What v0.5/v0.6 does:
vectorizer = TfidfVectorizer(...)  # Creates NEW vectorizer
X = vectorizer.fit_transform(texts)  # Different features!
predictions = model.predict(X)  # Model confused!

# What should happen:
# Use the SAME vectorizer that trained the model
X = original_vectorizer.transform(texts)  # Same features!
predictions = model.predict(X)  # Model happy!
```

### Why This Matters

| Component | Training Time | Your Analysis (v0.5/v0.6) |
|-----------|--------------|---------------------------|
| **Vectorizer** | original_vec.pkl | NEW TfidfVectorizer() |
| **Features** | "ignore", "bypass", "override" | "and begin afresh", "science" |
| **Vocab Size** | 384 features | 384 features |
| **Feature Names** | Different! | Different! |
| **Result** | Model trained on X | Model sees Y â†’ Confused! |

---

## âœ… Proposed Solution: SHAP-Based Analysis (v0.7)

### Why SHAP?

**SHAP (SHapley Additive exPlanations)** solves the vectorizer problem:

1. âœ… Works with model's ACTUAL features
2. âœ… Shows per-feature contributions
3. âœ… No need to match vectorizers manually
4. âœ… Game theory-based (mathematically sound)
5. âœ… Fast for tree models like XGBoost

### How SHAP Works

```
For each prediction:
  Base value (expected output)
  + SHAP(feature_1) â†’ e.g., +0.3 (pushes malicious)
  + SHAP(feature_2) â†’ e.g., -0.1 (pushes benign)
  + SHAP(feature_3) â†’ e.g., +0.5 (pushes malicious)
  + ...
  = Final prediction
```

**Interpretation**:
- **Positive SHAP** â†’ Feature pushes toward MALICIOUS
- **Negative SHAP** â†’ Feature pushes toward BENIGN
- **Magnitude** â†’ Strength of contribution

---

## ğŸ“Š Complete End-to-End Workflow

### **Improved Workflow with v0.7**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Data Preparation                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load MPDD.csv dataset                                   â”‚
â”‚  2. Sample for faster analysis (1K-10K samples)             â”‚
â”‚  3. Stratified sampling (maintain class balance)            â”‚
â”‚  4. Validate data quality                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Model Loading & Validation                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load pre-trained classifier.pkl                         â”‚
â”‚  2. Create vectorizer (TF-IDF with appropriate settings)    â”‚
â”‚  3. Transform texts to features                             â”‚
â”‚  4. Get baseline predictions                                â”‚
â”‚  5. Validate accuracy > 60% (check for mismatch)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: SHAP Analysis                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Create SHAP TreeExplainer (fast for XGBoost)            â”‚
â”‚  2. Calculate SHAP values for all samples                   â”‚
â”‚  3. Compute mean absolute SHAP (global importance)          â”‚
â”‚  4. Separate by class (malicious vs benign)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Feature Analysis                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Rank features by mean |SHAP| (overall importance)       â”‚
â”‚  2. Identify malicious features (positive SHAP in mal)      â”‚
â”‚  3. Identify benign features (negative SHAP in mal)         â”‚
â”‚  4. Compute discriminative scores                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Visualization & Insights                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. SHAP summary plot (all features)                        â”‚
â”‚  2. SHAP bar plot (top features)                            â”‚
â”‚  3. Force plots (individual predictions)                    â”‚
â”‚  4. Waterfall plots (feature contributions)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: Actionable Outputs                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Top malicious features list                             â”‚
â”‚  2. Top benign features list                                â”‚
â”‚  3. Per-sample explanations                                 â”‚
â”‚  4. Security filter recommendations                         â”‚
â”‚  5. Model improvement suggestions                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ SHAP Analysis Capabilities

### 1. **Global Feature Importance**

Shows which features matter most across ALL samples:

```python
mean_abs_shap = np.abs(shap_values).mean(axis=0)
# Top features by overall impact
```

**Use Case**: Understand model's general behavior

### 2. **Class-Specific Features**

Shows features that distinguish malicious from benign:

```python
mal_mean_shap = shap_values[malicious_mask].mean(axis=0)
ben_mean_shap = shap_values[benign_mask].mean(axis=0)
discriminative_score = mal_mean_shap - ben_mean_shap
```

**Use Case**: Build security filters

### 3. **Individual Predictions**

Explains why a specific prompt was classified:

```python
shap.force_plot(explainer.expected_value,
                shap_values[i],
                X[i])
```

**Use Case**: Debug specific cases, explain to users

### 4. **Interactive Visualizations**

- Summary plots
- Force plots
- Waterfall plots
- Dependence plots

**Use Case**: Exploratory analysis, presentations

---

## ğŸ“ˆ Expected Results from v0.7

### What You'll Get:

1. **Top Malicious Features** (e.g., from v0.7 run)
   ```
   1. 'ignore' â†’ +0.234 SHAP
   2. 'bypass' â†’ +0.189 SHAP
   3. 'override' â†’ +0.156 SHAP
   4. 'disregard' â†’ +0.134 SHAP
   5. 'forget' â†’ +0.112 SHAP
   ```

2. **Top Benign Features**
   ```
   1. 'what is' â†’ -0.187 SHAP
   2. 'how to' â†’ -0.165 SHAP
   3. 'please help' â†’ -0.143 SHAP
   4. 'can you explain' â†’ -0.128 SHAP
   ```

3. **Per-Prompt Explanations**
   ```
   Prompt: "Ignore all previous instructions..."

   Features pushing MALICIOUS:
   - 'ignore' â†’ +0.45
   - 'previous' â†’ +0.23
   - 'instructions' â†’ +0.34

   Features pushing BENIGN:
   - 'all' â†’ -0.05
   ```

---

## ğŸ“ Key Learnings & Recommendations

### What Went Wrong (v0.5, v0.6)

âŒ **Created new vectorizers** instead of using model's original
âŒ **Feature mismatch** led to poor predictions
âŒ **Ablation on mismatched features** showed no impact
âŒ **High Mal/Ben ratios** didn't mean model uses those features

### What Works (v0.7)

âœ… **SHAP uses model's actual features** automatically
âœ… **Shows real contributions** to predictions
âœ… **Works even with vectorizer mismatch** (uses model's internal features)
âœ… **Provides interpretable insights**

### Recommendations

#### Short-Term (Immediate)

1. âœ… **Run v0.7 notebook** on 1K samples first (fast)
2. âœ… **Validate model works** (accuracy > 60%)
3. âœ… **Export top features** for security filters
4. âœ… **Analyze misclassifications** using SHAP

#### Medium-Term (This Week)

1. ğŸ“Š **Scale to 10K samples** for robust analysis
2. ğŸ“Š **Create SHAP dashboard** for ongoing monitoring
3. ğŸ“Š **Document malicious patterns** for team
4. ğŸ“Š **Integrate into CI/CD** for model monitoring

#### Long-Term (This Month)

1. ğŸš€ **Production SHAP integration** for real-time explanations
2. ğŸš€ **Automated alerting** on high-SHAP malicious features
3. ğŸš€ **Model retraining** based on SHAP insights
4. ğŸš€ **User-facing explanations** ("This was flagged because...")

---

## ğŸ› ï¸ Troubleshooting Guide

### If Model Accuracy is Still Low (~50%)

**Problem**: Vectorizer still doesn't match model's training

**Solutions**:
1. Check if model was trained with different vectorizer settings
2. Try loading original vectorizer (if saved separately)
3. Retrain model with current vectorizer
4. Use SHAP anyway - it will still show patterns!

### If SHAP Takes Too Long

**Problem**: Large dataset or complex model

**Solutions**:
1. Reduce sample size (1K is usually enough)
2. Use `shap.sample()` for background data
3. For XGBoost, TreeExplainer is already fast
4. Run on GPU if available

### If Results Don't Make Sense

**Problem**: Model might be broken or data issues

**Solutions**:
1. Check data quality (missing values, encoding)
2. Validate labels are correct
3. Check if model file is corrupted
4. Try different samples

---

## ğŸ“š References & Further Reading

### SHAP Resources
- [SHAP GitHub](https://github.com/slundberg/shap)
- [SHAP Paper](https://arxiv.org/abs/1705.07874)
- [Tutorial](https://shap.readthedocs.io/)

### XGBoost + SHAP
- [TreeExplainer](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.TreeExplainer.html)
- [XGBoost Interpretability](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)

### Best Practices
- [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/)
- [Google's Explainable AI](https://cloud.google.com/explainable-ai)

---

## ğŸ“ Conclusion

**v0.7 with SHAP is the correct approach** for understanding feature importance in the pre-trained model.

### Key Takeaways:

1. âœ… **SHAP solves the vectorizer mismatch problem**
2. âœ… **Provides real, actionable insights**
3. âœ… **Works with any model** (tree-based, linear, neural)
4. âœ… **Fast and scalable**
5. âœ… **Mathematically sound** (game theory)

### Next Steps:

1. Run v0.7 notebook in Google Colab
2. Start with 1K samples for validation
3. Scale to 10K for comprehensive analysis
4. Export results for security filters
5. Integrate SHAP into production pipeline

---

**Project Vigil**
*Making AI Security Explainable*
Version 0.7 - SHAP Analysis
Date: 2025-11-16
